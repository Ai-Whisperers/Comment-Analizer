#!/usr/bin/env python3
"""
CÃ¡lculo de configuraciÃ³n Ã³ptima para archivos de 1000 comentarios
Target: Procesar 1000 comentarios en <60s manteniendo calidad y respetando rate limits
"""

import math

def calculate_optimal_config_for_1000_comments():
    """Calcular configuraciÃ³n Ã³ptima para 1000 comentarios"""
    
    print("ðŸŽ¯ OPTIMIZACIÃ“N PARA ARCHIVOS DE 1000 COMENTARIOS")
    print("=" * 55)
    
    # ParÃ¡metros base
    total_comments = 1000
    target_time_seconds = 60  # Target: 1 minuto mÃ¡ximo
    
    # OpenAI gpt-4o-mini limits (el modelo que usamos)
    rate_limit_requests_per_min = 200
    rate_limit_tokens_per_min = 2_000_000
    context_window = 16_384
    
    # ParÃ¡metros del sistema actual
    base_tokens = 1200  # JSON structure
    tokens_per_comment = 80  # Estimated
    buffer_percentage = 1.10  # 10% safety
    avg_request_time = 3.0  # seconds (incluye latency)
    
    print(f"ðŸ“Š Target: {total_comments} comentarios en <{target_time_seconds}s")
    print(f"ðŸ¤– Modelo: gpt-4o-mini")
    print(f"ðŸ”„ Rate limits: {rate_limit_requests_per_min} req/min, {rate_limit_tokens_per_min:,} tokens/min")
    print()
    
    # CÃLCULO 1: MÃ¡ximo comentarios por request (basado en context window)
    available_tokens = context_window - base_tokens
    max_comments_per_request = int(available_tokens / tokens_per_comment / buffer_percentage)
    
    print(f"ðŸ“ Context window: {context_window:,} tokens")
    print(f"ðŸ’¬ MÃ¡ximo comentarios/request: {max_comments_per_request}")
    
    # CÃLCULO 2: NÃºmero de requests necesarios para diferentes batch sizes
    batch_sizes = [40, 50, 60, 70, 80, 90, 100]
    scenarios = []
    
    print(f"\nðŸ“ˆ ANÃLISIS DE BATCH SIZES:")
    print("-" * 40)
    
    for batch_size in batch_sizes:
        if batch_size > max_comments_per_request:
            continue
            
        # Calcular requests necesarios
        num_requests = math.ceil(total_comments / batch_size)
        
        # Tokens por request
        tokens_per_request = base_tokens + (batch_size * tokens_per_comment * buffer_percentage)
        total_tokens = num_requests * tokens_per_request
        
        # Tiempo estimado
        estimated_time = num_requests * avg_request_time
        
        # Rate limit validation
        requests_per_minute = num_requests / (estimated_time / 60) if estimated_time > 0 else 999
        tokens_per_minute = total_tokens / (estimated_time / 60) if estimated_time > 0 else 999
        
        within_rate_limits = (
            requests_per_minute <= rate_limit_requests_per_min and
            tokens_per_minute <= rate_limit_tokens_per_min
        )
        
        meets_target = estimated_time <= target_time_seconds
        
        scenario = {
            'batch_size': batch_size,
            'num_requests': num_requests,
            'tokens_per_request': int(tokens_per_request),
            'total_tokens': int(total_tokens),
            'estimated_time': estimated_time,
            'meets_target': meets_target,
            'within_rate_limits': within_rate_limits,
            'comments_per_second': total_comments / estimated_time,
            'requests_per_minute': requests_per_minute,
            'tokens_per_minute': tokens_per_minute
        }
        
        scenarios.append(scenario)
        
        # Display scenario
        status = "âœ…" if meets_target and within_rate_limits else "âŒ"
        print(f"{status} Batch {batch_size:2d}: {num_requests:2d} requests, {estimated_time:4.1f}s, {scenario['comments_per_second']:4.1f} c/s")
    
    # Find optimal scenarios
    valid_scenarios = [s for s in scenarios if s['meets_target'] and s['within_rate_limits']]
    
    if valid_scenarios:
        # Sort by efficiency (comments per second)
        valid_scenarios.sort(key=lambda x: x['comments_per_second'], reverse=True)
        
        print(f"\nðŸ† TOP CONFIGURACIONES VÃLIDAS:")
        print("-" * 35)
        
        for i, scenario in enumerate(valid_scenarios[:3], 1):
            print(f"{i}. ðŸ“¦ Batch size: {scenario['batch_size']}")
            print(f"   ðŸ”„ Requests: {scenario['num_requests']}")
            print(f"   ðŸ• Tiempo: {scenario['estimated_time']:.1f}s")
            print(f"   âš¡ Eficiencia: {scenario['comments_per_second']:.1f} comentarios/s")
            print(f"   ðŸŽ¯ Tokens/req: {scenario['tokens_per_request']:,}")
            print()
        
        # RecomendaciÃ³n
        best = valid_scenarios[0]
        print(f"ðŸŽ¯ RECOMENDACIÃ“N Ã“PTIMA PARA 1000 COMENTARIOS:")
        print(f"  ðŸ“¦ MAX_COMMENTS_PER_BATCH = {best['batch_size']}")
        print(f"  ðŸŽ¯ OPENAI_MAX_TOKENS = {best['tokens_per_request'] + 1000}")  # Buffer extra
        print(f"  âš¡ Performance esperado: {best['estimated_time']:.1f}s para 1000 comentarios")
        print(f"  ðŸ’° Requests necesarios: {best['num_requests']} (costo ~${best['num_requests'] * 0.004:.3f})")
        
        return best
    
    else:
        print("âŒ NO SE ENCONTRARON CONFIGURACIONES VÃLIDAS")
        print("ðŸ”§ Considerar:")
        print("  - Reducir batch size")
        print("  - Aumentar target time")
        print("  - Usar modelo con mayor context window")
        return None

def analyze_current_vs_optimal():
    """Comparar configuraciÃ³n actual vs Ã³ptima"""
    
    print(f"\nðŸ” COMPARACIÃ“N: ACTUAL vs Ã“PTIMA")
    print("=" * 40)
    
    # ConfiguraciÃ³n actual
    current_batch = 60
    current_max_tokens = 12000
    
    # ConfiguraciÃ³n Ã³ptima calculada
    optimal = calculate_optimal_config_for_1000_comments()
    
    if optimal:
        print(f"\nðŸ“Š CONFIGURACIÃ“N ACTUAL:")
        print(f"  ðŸ“¦ Batch size: {current_batch}")
        print(f"  ðŸŽ¯ Max tokens: {current_max_tokens:,}")
        
        # Calcular performance actual
        num_requests_current = math.ceil(1000 / current_batch)
        estimated_time_current = num_requests_current * 3.0  # 3s por request
        
        print(f"  ðŸ”„ Requests para 1000: {num_requests_current}")
        print(f"  â±ï¸ Tiempo estimado: {estimated_time_current:.1f}s")
        
        print(f"\nðŸ“Š CONFIGURACIÃ“N Ã“PTIMA:")
        print(f"  ðŸ“¦ Batch size: {optimal['batch_size']}")
        print(f"  ðŸŽ¯ Max tokens: {optimal['tokens_per_request'] + 1000:,}")
        print(f"  ðŸ”„ Requests para 1000: {optimal['num_requests']}")
        print(f"  â±ï¸ Tiempo estimado: {optimal['estimated_time']:.1f}s")
        
        # ComparaciÃ³n
        improvement_ratio = estimated_time_current / optimal['estimated_time']
        print(f"\nðŸ’¡ MEJORA POTENCIAL:")
        print(f"  âš¡ Speedup: {improvement_ratio:.1f}x mÃ¡s rÃ¡pido")
        print(f"  ðŸ“‰ Requests: {num_requests_current} â†’ {optimal['num_requests']} ({num_requests_current - optimal['num_requests']:+d})")
        
        if improvement_ratio > 1.2:
            print("ðŸš€ RECOMENDACIÃ“N: Aplicar configuraciÃ³n Ã³ptima")
        else:
            print("âœ… RECOMENDACIÃ“N: ConfiguraciÃ³n actual estÃ¡ bien optimizada")

if __name__ == "__main__":
    optimal_config = calculate_optimal_config_for_1000_comments()
    analyze_current_vs_optimal()
    
    if optimal_config:
        print(f"\nðŸ”§ PARA APLICAR EN SECRETS.TOML:")
        print(f'MAX_COMMENTS_PER_BATCH = "{optimal_config["batch_size"]}"')
        print(f'OPENAI_MAX_TOKENS = "{optimal_config["tokens_per_request"] + 1000}"')
    
    print("\nâœ… AnÃ¡lisis completado")